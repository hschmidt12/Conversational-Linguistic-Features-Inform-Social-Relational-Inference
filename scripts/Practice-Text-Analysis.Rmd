---
title: "Practice - Text Analysis"
author: "Helen Schmidt"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: false
---
<style>
h1, h2, h3, h4, h5, h6, legend {
    color: #046C9A;
}
</style>

December 2021 - HS experimenting with some sentiment analysis packages!

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(dplyr)
library(knitr)
library(readr)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(tm)
library(SnowballC)
library(XML)
library(RCurl)
library(ggpubr)
library(wesanderson)
library(reshape2)
library(MetBrewer)
```

```{r message=FALSE, warning=FALSE}
# define color palette
colors <- met.brewer("Robert", 6, type = "discrete")
palette <- met.brewer("Robert", 25, type = "continuous")
```

# Basic test - sentiment analysis

Adapted from a tutorial on how to get started with SentimentR package (found at https://github.com/amrrs/youtube-r-snippets/blob/master/sentimentr_demo.R). 

SentimentR uses Jockers 2017 dictionary.


#### What does 'sentiment' output look like for some example sentences?

"This tutorial is awesome. I really need coffee today. Coffee is my favorite thing besides this tutorial."

```{r message=FALSE, warning=FALSE}
library(sentimentr)

text <- "This tutorial is awesome. I really need coffee today. Coffee is my favorite thing besides this tutorial."
sentiment(text)
```

Using *sentiment*, each sentence is assigned a 'sentiment' score. The larger the number, the more positive the sentiment. The more negative the number, the more negative the sentiment. If the sentiment score is neutral, the sentence has neutral sentiment. I can also see the number of words in each sentence.

```{r message=FALSE, warning=FALSE}
sentiment_by(text, by = NULL)
```

I can also look at averages across all three sentences using *sentiment_by*. This will give me the total number of words across all three sentences, the SD in number of words between sentences, and the average sentiment score across all three sentences.

```{r message=FALSE, warning=FALSE}
profanity(text)
```

If I need to, I can also count how many profanities occur in each sentence!

***

# Presidental debate (2012) data

#### Now let's look at a bigger dataset. 

I'm going to load the presidential debate data from 2012 (a base R dataset). I will run the same *sentiment*, *sentiment_by*, and *profanity* functions on the dialogue vector contained in the debate data.

I'll also add the sentiment polarity levels (positive, neutral, or negative sentiment) into my main **debates** dataframe. If my sentiment score is above 0, I'll label the sentiment "positive". If it's below 0, I'll label the sentiment "negative". If the score is exactly 0, I'll label the sentiment "neutral".

Romney and Obama are the presidential candidates. Lehrer, Crowley, and Schieffer are moderators for each debate (debates 1, 2, and 3 respectively). Question refers to someone else asking the candidates a question during the debate.

```{r message=FALSE, warning=FALSE}
# load debate data
debates <- presidential_debates_2012

sentiment(debates$dialogue)
sentiment_by(debates$dialogue, by = NULL)
profanity(debates$dialogue)

# add polarity levels for each sentence
debates_with_pol <- debates %>% 
  get_sentences() %>% #get each sentence
  sentiment() %>% #get sentiment score
  mutate(polarity_level = ifelse(sentiment < 0, "Negative", #make scores < 0 "negative" and scores > 0 "positive"
                                 ifelse(sentiment > 0, "Positive","Neutral"))) #else, make scores = 0 "neutral"

```

#### Plot average sentiment score per person

Let's see what each speaker's average sentiment score (across all sentences) is!

```{r message=FALSE, warning=FALSE}
ggplot(debates_with_pol, aes(x = person, y = sentiment, fill = person)) + 
  stat_summary(fun = mean, geom = "bar", color = "black", alpha = 0.8, position = "dodge") +
    stat_summary(fun.data = mean_se, geom = "errorbar", fun.args = list(mult = 1.96), width = 0.3, color = "black") +
  scale_fill_manual(values = colors) +
  xlab("person") + ylab("average sentiment") + 
  theme_classic() + theme(legend.position = "none") 
```

#### Plot average word count per person

How many words, on average, is each speaker using in a sentence?

```{r message=FALSE, warning=FALSE}
ggplot(debates_with_pol, aes(x = person, y = word_count, fill = person)) + 
  stat_summary(fun = mean, geom = "bar", color = "black", alpha = 0.8, position = "dodge") +
    stat_summary(fun.data = mean_se, geom = "errorbar", fun.args = list(mult = 1.96), width = 0.3, color = "black") +
  scale_fill_manual(values = colors) +
  xlab("person") + ylab("average word count per sentence") + 
  theme_classic() + theme(legend.position = "none") 

```

#### Plot sentiment density per person

What's the distribution of sentiment scores per person?

```{r message=FALSE, warning=FALSE}

ggplot(debates_with_pol, aes(x=sentiment,color=person)) + facet_wrap(vars(person)) +
  geom_vline(xintercept=0, linetype='dashed', color='grey') +
  geom_density(aes(sentiment)) +
  scale_color_manual(values = colors) +
  theme_classic() + theme(legend.position = "none") 
 
```

### Candidates only

#### Plot average sentiment score at each debate 

Now I want to examine sentiment scores across the three debates. Now I'll only include Romney and Obama, so I'll first subset my data to include only sentences spoken by them. Let's see each candidate's average sentiment score (across all sentences) in each debate!

```{r message=FALSE, warning=FALSE}
RomneyObama <- subset(debates_with_pol, debates_with_pol$person == c("ROMNEY","OBAMA"))

ggplot(RomneyObama, aes(x = time, y = sentiment, fill = time)) + facet_wrap(vars(person)) +
  stat_summary(fun = mean, geom = "bar", color = "black", alpha = 0.8, position = "dodge") +
    stat_summary(fun.data = mean_se, geom = "errorbar", fun.args = list(mult = 1.96), width = 0.3, color = "black") +
  scale_fill_manual(values = colors) +
  xlab("debate") + ylab("average sentiment") + 
  theme_classic() + theme(legend.position = "none") 

```

What percent of each candidate's dialogue is negative vs. neutral vs. positive?

```{r message=FALSE, warning=FALSE}
RomneyObama <- subset(debates_with_pol, debates_with_pol$person == c("ROMNEY","OBAMA"))

polarity.colors <- c("firebrick","gold2","springgreen4")

ggplot(RomneyObama, aes(x = time, fill = polarity_level)) + facet_wrap(vars(person)) +
  geom_bar(position = "fill", alpha = 0.8) +
  scale_fill_manual(values = polarity.colors) + 
  scale_y_continuous(labels = scales::percent) +
  xlab("debate") + ylab("sentiment") +
  theme_classic() 
```


#### Plot average word count at each debate

How many words, on average, is each speaker using in a sentence?

```{r message=FALSE, warning=FALSE}
RomneyObama <- subset(debates_with_pol, debates_with_pol$person == c("ROMNEY","OBAMA"))

ggplot(RomneyObama, aes(x = time, y = word_count, fill = time)) + facet_wrap(vars(person)) +
  stat_summary(fun = mean, geom = "bar", color = "black", alpha = 0.8, position = "dodge") +
    stat_summary(fun.data = mean_se, geom = "errorbar", fun.args = list(mult = 1.96), width = 0.3, color = "black") +
  scale_fill_manual(values = colors) +
  xlab("debate") + ylab("average word count per sentence") + 
  theme_classic() + theme(legend.position = "none") 

```

#### Plot sentiment density at each debate

What's the distribution of sentiment scores per candidate at each debate?

```{r message=FALSE, warning=FALSE}
RomneyObama <- subset(debates_with_pol, debates_with_pol$person == c("ROMNEY","OBAMA"))

ggplot(RomneyObama, aes(x=sentiment,color=person)) + facet_grid(rows= vars(person), cols = vars(time)) +
  geom_vline(xintercept=0, linetype='dashed', color='grey') +
  geom_density(aes(sentiment)) +
  scale_color_manual(values = colors) +
  theme_classic() + theme(legend.position = "none") 
 
```

# Social networks

One idea I have for the Survivor data is to see how similarly a social network created JUST using the number of spoken interactions between players is to a social network defined by participant responses (choosing friends, rivals, winners, etc.)
To practice creating a social network defined by character interactions, I'm following a network analysis tutorial, courtesy of the Language Technology and Data Analysis Lab @ the University of Queensland (https://slcladal.github.io/net.html).

```{r message=FALSE, warning=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# activate packages
library(flextable)
library(GGally)
library(ggraph)
library(gutenbergr)
library(igraph)
library(Matrix)
library(network)
library(quanteda)
library(sna)
library(tidygraph)
library(tidyverse)
library(tm)
library(tibble)
# activate klippy for copy-to-clipboard button
klippy::klippy()
```

### Load Data

I want to create a network that shows how often characters in William Shakespeare's "Romeo and Juliet" appeared in the same scene together. I can do this by visualizing the network by defining my nodes (represented as dots) and my edges (represented as lines between dots). 

In directed networks, the direction of edges is captured and will have arrows to indicate direction. The thickness of lines can also be used to show information such as frequency. 

First, I want to load in the Romeo and Juliet data, and reshape it into a co-occurrence matrix (showing how often two characters appear in the same scene for all characters). 

*Helen note - this step has messy code; find a way to make it cleaner!*

```{r}
# load data
rom <- read.delim("https://slcladal.github.io/data/romeo_tidy.txt", sep = "\t")

# reshape data 
test <- rom[,1:2] #take only act/scene and person info
test <- dcast(test,actscene~person) #reshape from long data into wide data format
ActScene <- data.frame(test[,1]) #save act/scene info
test <- test[,-1]
test[!is.na(test)] <- 1
test[is.na(test)] <- 0
test <- mutate_all(test, function(x) as.numeric(as.character(x)))

# create co-occurrence matrix
romeo <- as.matrix(test)
out <- crossprod(romeo)  # Same as: t(X) %*% X
diag(out) <- 0       # (b/c you don't count co-occurrences of an aspect with itself)
romeo <- data.frame(out)

```

#### Define nodes and edges

Now that I have a co-occurrence matrix, I can define my nodes and edges. Using network graph functions from *igraph*, *ggraph*, and *tidygraph* packages, I can use these nodes and edges to visualize the network. 

First, let's define the nodes...

```{r}
# define nodes using co-occurence matrix
va <- romeo %>%
  dplyr::mutate(Persona = rownames(.),
                Occurrences = rowSums(.)) %>%
  dplyr::select(Persona, Occurrences) %>%
  dplyr::filter(!str_detect(Persona, "SCENE"))
```

Now, let's define the edges...

```{r}
ed <- romeo %>%
  dplyr::mutate(from = rownames(.)) %>%
  tidyr::gather(to, Frequency, BALTHASAR:TYBALT) %>%
  dplyr::mutate(Frequency = ifelse(Frequency == 0, NA, Frequency))
```

Now that I have generated tables for nodes and edges, I can generate a graph object and add labels to my nodes.

```{r message=FALSE, warning=FALSE}
ig <- igraph::graph_from_data_frame(d=ed, vertices=va$vertices, directed = FALSE)

tg <- tidygraph::as_tbl_graph(ig) %>% 
  tidygraph::activate(nodes) %>% 
  dplyr::mutate(label=name)

# set seed
set.seed(12345)
# edge size shows frequency of co-occurrence
tg %>%
   ggraph(layout = "fr") +
   geom_edge_arc(colour= "gray50",
                 lineend = "round",
                 strength = .1,
                 alpha = .1) +
   geom_node_text(aes(label = name), 
                  repel = TRUE, 
                  point.padding = unit(0.2, "lines"), 
                  colour="gray10") +
  theme_graph(background = "white") +
  guides(edge_width = FALSE,
         edge_alpha = FALSE)

#v.size <- V(tg)$occurrences #define vertex (node) size
# inspect
v.size <- c(9,34,46,14,12,20,36,45,15,22,38,21,9,22,54,16,15,22)
v.size
```

```{r}

E(tg)$weight <- E(tg)$Frequency
# inspect weights
head(E(tg)$weight, 10)

# define colors (by family)
mon <- c("ABRAM", "BALTHASAR", "BENVOLIO", "LADY MONTAGUE", "MONTAGUE", "ROMEO")
cap <- c("CAPULET", "CAPULET’S COUSIN", "FIRST SERVANT", "GREGORY", "JULIET", "LADY CAPULET", "NURSE", "PETER", "SAMPSON", "TYBALT")
oth <- c("APOTHECARY", "CHORUS", "FIRST CITIZEN", "FIRST MUSICIAN", "FIRST WATCH", "FRIAR JOHN" , "FRIAR LAWRENCE", "MERCUTIO", "PAGE", "PARIS", "PRINCE", "SECOND MUSICIAN", "SECOND SERVANT", "SECOND WATCH", "SERVANT", "THIRD MUSICIAN")
# create color vectors
Family <- dplyr::case_when(sapply(tg, "[")$nodes$name %in% mon ~ "MONTAGUE",
                           sapply(tg, "[")$nodes$name %in% cap ~ "CAPULET",
                           TRUE ~ "Other")
# inspect colors
Family

# set seed
set.seed(12345)
# edge size shows frequency of co-occurrence
tg %>%
   ggraph(layout = "fr") +
   geom_edge_arc(colour= "gray50",
                  lineend = "round",
                 strength = .1,
                 aes(edge_width = weight,
                     alpha = weight)) +
   geom_node_point(size=2, 
                   aes(color=Family)) +
   geom_node_text(aes(label = name), 
                  repel = TRUE, 
                  point.padding = unit(0.2, "lines"), 
                  size=4, 
                  colour="gray10") +
  scale_edge_width(range = c(0, 2.5)) +
  scale_edge_alpha(range = c(0, .3)) +
  theme_graph(background = "white") +
  theme(legend.position = "top") +
  guides(edge_width = FALSE,
         edge_alpha = FALSE)
```






