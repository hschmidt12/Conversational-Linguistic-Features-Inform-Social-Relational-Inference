---
title: "Universal Sentence Encoder - Season 8 Survivor"
author: "Helen Schmidt"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: false
---
<style>
h1, h2, h3, h4, h5, h6, legend {
    color: #046C9A;
}
</style>

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(dplyr)
library(knitr)
library(readr)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(tm)
library(SnowballC)
library(XML)
library(RCurl)
library(ggpubr)
library(wesanderson)
library(reshape2)
library(MetBrewer)
library(sentimentr)
library(sna)
library(splitstackshape)
library(GGally)
library(ggraph)
library(gutenbergr)
library(igraph)
library(Matrix)
library(network)
library(tidygraph)
library(tibble)
library(reactable)
library(reactablefmtr)
```

***

Group all sentences by one speaker and calculate overall embedding score for each speaker. Compare speakers in heatmap!

# Semantic Similarity with TF-Hub Universal Sentence Encoder

```{r}
library(reticulate)
py_config()
py_install("seaborn")
py_install("tensorflow")
py_install("tensorflow_hub",pip=T)
py_install("torch",pip=T)
```

```{r}
# read in transcription csv
## CHANGE BASED ON EPISODE NUMBER ##
dialogue <- read.csv("/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Transcriptions/Survivor Transcriptions - Ep. 2.csv")
# remove clip number column
dialogue <- dialogue[,-1]
# rename recipient column
names(dialogue)[3] <- "Recipient"
# make sure names are consistent
dialogue$Speaker <- ifelse(grepl("Rob C", dialogue$Speaker), "Rob C.", 
                                 ifelse(grepl("Rob M", dialogue$Speaker), "Rob M.",
                                        ifelse(grepl("Jenna M", dialogue$Speaker), "Jenna M.",
                                               ifelse(grepl("Jenna L", dialogue$Speaker), "Jenna L.",
                                                      ifelse(grepl("Shii Ann", dialogue$Speaker), "ShiiAnn", dialogue$Speaker)))))

# filter such that I only include all 18 contestants as possible speakers
filter.dialogue <- filter(dialogue, c(Speaker == "Rob C." | Speaker == "Sue" |
                                        Speaker == "Alicia" | Speaker == "Tom" |
                                        Speaker == "Rob M." | Speaker == "Amber" |
                                        Speaker == "Jenna M." | Speaker == "Richard" |
                                        Speaker == "Colby" | Speaker == "Lex" |
                                        Speaker == "Kathy" | Speaker == "ShiiAnn" |
                                        Speaker == "Tina" | Speaker == "Rudy" |
                                        Speaker == "Ethan" | Speaker == "Jerri" |
                                        Speaker == "Rupert" | Speaker == "Jenna L.")) 


# clean transcriptions so I have one observation per sentence of dialogue
filter.dialogue <- filter.dialogue %>% 
  get_sentences() %>% #get each sentence
  sentiment() # get sentiment score and word count per sentence too
# add order count of sentences
filter.dialogue$order <- 1:nrow(filter.dialogue)

# plot dialogue word counts for each speaker
pp <- met.brewer(name = "Hiroshige", n = 20)
pp_vec <- c(pp)
p1 <- ggplot(filter.dialogue, aes(x = order, y = word_count, color = Speaker)) +
  geom_line(aes(group = Speaker), alpha = .3) +
  geom_smooth(aes(group = Speaker), se = FALSE) +
  ylab("word count") +
  ylim(0, 40) +
  theme_classic() +
  ggtitle("Character word count over the course of the episode") +
  labs(x = "Time") +
  scale_color_manual(values = pp_vec) +
  theme(panel.background = element_blank())
p1 #plot!

# save split sentences to use in USE (lol)
## CHANGE BASED ON EPISODE NUMBER ##
write.csv(filter.dialogue, file = "/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Transcriptions/Split-Sentences/ss-Ep2.csv")

ggsave(p1, file = '/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Results/WordCount/Ep2_wordcount.png')
```


```{python}
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import torch

embed = hub.load("universal-sentence-encoder_4")

# read in survivor text data (split into single sentences)
## CHANGE BASED ON EPISODE NUMBER ##
df = pd.read_csv("/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Transcriptions/Split-Sentences/ss-Ep2.csv")

# save dialogue as new series
sentences = df.Text
# run embeddings on all sentences (regardless of speaker)
sentence_embedding = embed(sentences)

# get list of unique speakers in this episode
speakers = df.Speaker.unique()
# alphabetize list
speakers.sort()
# print list
speakers 
```

# Dimension scores for individual speakers

```{python}
# loop through all speakers and save 1) just their dialogue and calculate 2) their word embeddings
speaker_dialogue = []
speaker_embedding = []
appended_dialogue = []
appended_embeddings =[]

for x in speakers:
  print(x)
  currentSpeaker = str(x)
  # get only this current speaker's sentences from the whole episode
  speaker = df[df["Speaker"] == currentSpeaker]
  # concatenate all their sentences into one cell
  currentDialogue = "  ".join(speaker.Text)
  currentDialogue = [currentDialogue]
  # create data frame with all of speaker's dialogue in one cell
  speaker_dialogue = {'Speaker':[currentSpeaker],
                    'Dialogue':[currentDialogue]}
  speaker_dialogue = pd.DataFrame(speaker_dialogue)
  # store dialogue in a list
  appended_dialogue.append(speaker_dialogue)
  
  # run embedding on all of this speaker's dialogue
  speaker_embedding = embed(currentDialogue)
  # store embeddings in a list
  appended_embeddings.append(speaker_embedding)
  
  # combine all speakers' dialogues into a new data frame
  all_dialogue = pd.concat(appended_dialogue)
  # combine all embeddings into one tensor shape (# of speakers X 512)
  speakers_embedding = tf.concat((appended_embeddings),0)
else:
  print("done!")

speakers = pd.DataFrame(speakers)
# save speaker list in dummy csv (write over each time) just so I can pull into R to make the correlation table later
speakers.to_csv('/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/dummy_speakers.csv')
```


## Plot!

How similar are speakers to each other in the episode?

```{python}
# now I have a tensor shape with all dimension scores for all speakers (speakers_embedding)
# I also have the dialogue from each speaker (all_dialogue)

# using np.inner, I calculate the inner product of vectors contained within the embedding arrays

# plot a heatmap of similarity between speakers
def plot_similarity(labels, features, rotation): # labels = speakers, features = 512 scores, rotation = angle of text
  corr = np.inner(features, features)            # correlate 512 dimension scores across speakers
  #print(corr)                                    # print correlation matrix (speakers X speakers)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlGnBu")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Similarity")
  plt.tight_layout()
  plt.savefig("/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Results/Heatmaps/Ep2_similarity.png")
  plt.show()

plot_similarity(all_dialogue['Speaker'],speakers_embedding, 70)

## CHANGE BASED ON EPISODE NUMBER ##
corr = pd.DataFrame(np.inner(speakers_embedding, speakers_embedding)) # get correlation matrix
corr.to_csv('/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Results/Correlations/raw/raw_corr_Ep2.csv')
```

### Save correlation matrix

```{r}
speakers <- read.csv('/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/dummy_speakers.csv')

## CHANGE CORR BASED ON EPISODE NUMBER ##
corr <- read.csv('/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Results/Correlations/raw/raw_corr_Ep2.csv')

# delete first column of corr and make speakers a vector of names
speakers <- speakers[,-1]
corr <- corr[,-1]

col.names <- speakers
row.names <- speakers

names(corr)[1:length(corr)] <- col.names
rownames(corr) <- row.names

## CHANGE BASED ON EPISODE NUMBER ##
# save for modeling
write.csv(corr,"/Users/tuo70125/My Drive/SANLab/Experiments/Survivor-Language/Analysis/Whole-Season/Results/Correlations/Ep2_corr.csv")

# print nice reactable for correlations
reactable(corr,
          outlined = T, striped = T,
          style = list(fontFamily = "Avenir", fontSize = "12px"),
          compact = T,
          resizable = T,
          wrap = F)
```

